[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Antonio (He/Him/His)\nDeploying and managing IT services in AWS | Project Manager | Cloud Architect (Networking) | Infrastructure as Code"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/lambda-deployment-types/index.html",
    "href": "posts/lambda-deployment-types/index.html",
    "title": "Different strategies for AWS Lambdas deployments with Terraform",
    "section": "",
    "text": "You have done several AWS Lambda deployments using Terraform (TF), and you are starting to do deployments in AWS using Terraform Cloud (TFC). Because you know there will be more Lambda deployments soon, you want to be proactive, think strategically, and implement a workflow to sustain your future workload, provide resilience, and leave room for integrations with other technologies.\nWhen I started with Lambda deployments, I quickly jumped on deployment after deployment without too much thought into the future; there was a sense of urgency in migrating IT services to the cloud ASAP. I wished I had slowed down and planned better in architecting all moving parts and their interactions. If I had put some thought in, it would have prevented me from several refactors to address old deployments’ inherent limitations.\nI will list some approaches I used for lambda deployments and how they evolved to overcome the limitations I encountered.\n\nAll in One: Deployment Package included with the TF Configuration Files\nThis approach is the simplest, and everybody follows it while learning lambda to use the tool. The aws_lambda_function terraform stanza in main.tf file does not look complicated, and it is easy to follow what TF will do. The code should look similar to the example below (I am going to be minimalistic, showing only the relevant arguments in every instance):\nresource \"aws_lambda_function\" \"this\" { \n    ...\n    function_name    = \"lambda_name\"\n    filename         = \"deployment_package.zip\" \n    source_code_hash = filebase64sha256(\"deployment_package.zip\")\n    runtime          = \"python3.11\"\n    ...\n}\nDespite being a common approach, it has several limitations. You need to include in the repository the deployment package, which is a ZIP file. It is not entirely bad, but it is not recommended. Storing compressed binary code in the PR can make the approval process unreliable. Approvers will check the source code and trust that the ZIP file was properly created. (If you are tracking the source code elsewhere, then a PR for the ZIP is useless.)\n\n\nLeveraging the archive Provider to Create the Deployment Package\nOne way to not need the deployment package in the repository is with the archive provider. What this provider does is TFC will create the deployment package during the apply. This simplifies the PRs because it contains source code that approvers can check. In the main.tf, you must add the archive_file data source and modify the aws_lambda_function resource to reference the relevant archive_file attributes. The configuration file should look like this:\nresource \"aws_lambda_function\" \"this\" { \n    ...\n    filename         = data.archive_file.demo_lambda_zip.output_path \n    source_code_hash = data.archive_file.this.output_base64sha256 \n    ...\n}\nand\ndata \"archive_file\" \"this\" { \n    ...\n    type        = \"zip\" \n    source_file = \"${path.module}/src/application.py\"  \n    output_path = \"${path.module}/zip/deployment_package.zip\" \n    ...\n}\nUsing the archive provider addressed your previous issue of not needing to include the deployment package in the repository. Unfortunately, it does not offer a good advantage if the lambda requires dependencies, and the most common dependencies I have seen for lambda deployments are requests and boto3. If you decide to add the dependencies with the source code, the pull request will contain too many files, making the approvers’ job more difficult and running limitations of the version control GUI to list all the changes.\n\n\nStoring the Deployment Package in a S3 Bucket\nThe other option for uploading the deployment package into the AWS Lambda service is a S3 bucket. The S3 bucket stores the deployment package, and you configure the corresponding arguments in the aws_s3_bucket resource to use the bucket.\nCreate the bucket in a separate terraform deployment if you include the aws_s3_bucket resource and the rest of the resources in the main.tf, you will run into drifting again.\nresource \"aws_lambda_function\" \"lambda\" { \n    ...\n    s3_bucket        = \"bucket-deployment-package-lambda\" \n    s3_key           = \"deployment_package.zip\" \n    source_code_hash = base64sha256(file(\"deployment_package.zip\")) \n    ... \n}\nI prefer using the aws_s3_object data source instead to take advantage of its arguments; thus, the code looks more like this:\nresource \"aws_lambda_function\" \"lambda\" { \n    ...\n    s3_bucket        = data.aws_s3_object.lambda.bucket \n    s3_key           = data.aws_s3_object.lambda.key \n    source_code_hash = data.aws_s3_object.lambda.etag\n    ... \n}\ndata \"aws_s3_object\" \"lambda\" { \n    ...\n    bucket = \"bucket-deployment-package-lambda\" \n    key    = \"deployment_package.zip\"\n    ...\n}\nThis approach separates the lambda code from the terraform configuration files. Now, it is easier to develop automation for the lambda code that does not conflict with Terraform Cloud workflow.\n\n\nWorking with S3 Buckets Encryption\nMy success was short-lived for the reason that I overlooked encrypting the bucket. The side effect of an encrypted bucket is the etag attribute fails because the MD5 function can no longer access the file’s content. Luckily, the change is trivial and only requires replacing this argument with s3_object_version; versioning must also be enabled.\nresource \"aws_lambda_function\" \"lambda\" { \n    ...\n    s3_bucket         = data.aws_s3_object.lambda.bucket \n    s3_key            = data.aws_s3_object.lambda.key \n    s3_object_version = data.aws_s3_object.lambda.version_id \n    ... \n}\n\n\nUsing Lambda Layers to the Deployment\nYou probably already saw the following message when you tried to view the Lambda console code editor: The deployment package of your Lambda function REDACTED is too large to enable inline code editing… The most probable culprits are the dependencies making the deployment package larger than 3MB. It is acceptable to leave the lambda as it is; however, having access to the source code is convenient for quick troubleshooting.\nUsing lambda layers can provide some help. Instead of bundling the lambda code with dependencies, you can create two deployment packages: one for the lambda code and the other for your lambda layer. Both can be stored in the same S3 bucket if preferred.\nFirst, I add the aws_lambda_layer_version resource to the main.tf to create the layer:\nresource \"aws_lambda_layer_version\" \"layer\" {\n    ...\n    layer_name          = \"lambda_layer_name\"\n    s3_bucket           = data.aws_s3_object.layer.bucket\n    s3_key              = data.aws_s3_object.layer.key\n    s3_object_version   = data.aws_s3_object.layer.version_id\n    compatible_runtimes = [\"python3.11\"]\n    ...\n}\ndata \"aws_s3_object\" \"layer\" {\n    ...\n    bucket = \"bucket-deployment-package-lambda\"\n    key    = \"layer_deployment_package.zip\"\n    ...\n}\nThen, I changed the aws_lambda_function resource to include a reference to the layer.\nresource \"aws_lambda_function\" \"lambda\" {\n    ...\n    layers = [aws_lambda_layer_version.layer.arn]\n    ...\n}\nAside from the extra complexity and developing time, lambda layers bring benefits. It reduces the size of your deployment packages, separates core function logic from dependencies, shares dependencies, and allows using the Lambda console code editor.\nYou must note that you can manage independent pipelines for each component: terraform configuration files, lambda code, and dependencies. There is no need to create a one-size-fits-all-convoluted pipeline for a deployment. The test scripts can be more straightforward, and testing should be faster since you only need to check the item changed instead of the entire deployment.\n\n\nIn the end…\nUltimately, you and your team must evaluate and decide on a strategy that fits appropriately your environment based on workflow, toolings, knowledge, etc. My goal here is list and comment on the approaches I have worked with."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Different strategies for AWS Lambdas deployments with Terraform\n\n\n\n\n\n\n\ntf\n\n\ncode\n\n\nlambda\n\n\naws\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nAntonio\n\n\n\n\n\n\nNo matching items"
  }
]