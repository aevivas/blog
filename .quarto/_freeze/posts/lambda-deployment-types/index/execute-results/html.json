{
  "hash": "221f4c030c07b5609ece0c3b539d05a1",
  "result": {
    "markdown": "---\ntitle: \"Different strategies for AWS Lambdas deployments with Terraform\"\nauthor: \"Antonio\"\ndate: \"2023-10-04\"\ncategories: [tf, code, lambda, aws]\n---\n\nYou have done several AWS Lambda deployments using using Terraform (TF), and you are starting to do deploymnents in AWS using Terraform Cloud (TFC). Because you know there will be more Lambda deployments in the near future, you want to be proactive, think strategically, and implement a workflow to sustain your future workload, provide resilience, and leave room for integrations with other technologies.\n\nWhen I started with Lambda deployments, I quickly jumped on deployment after deployment without to much thought into the future; there was a sense of urgency in migrating IT services to the cloud ASAP. I wished I had slowed down and thought more about better architecting all moving parts and their interactions. This would have prevented me from several refactors just to address old deployments' inherent limitation.\n\nI will list the different approaches I have used for lambda deployments and how they evolved to overcome the limitations.\n\n# All in One: Deployment Package included with the TF Configuration Files\n\nThis approach is the simplest, and everybody follows it while learning lambda to use the tool. The *aws_lambda_function* terraform stanza in *main.tf* file does not look complicated, and it is easy to follow what TF will do. The code should look similar to the example below (I am going to be minimalistic showing only the relevant arguments in every example):\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\na = 2\nb = 3\nc = a + b\n\nprintf(c)\n```\n:::\n\n\n```default\nresource \"aws_lambda_function\" \"this\" { \n    ...\n    function_name    = \"lambda_name\"\n    filename         = \"deployment_package.zip\" \n    source_code_hash = filebase64sha256(\"deployment_package.zip\")\n    runtime          = \"python3.11\"\n    ...\n}\n```\n\nDespite being a common approach, it has several limitations. You need to include in the repository the deployment package, which is a ZIP file. It is not entirely bad, but it is not recommended. Storing compressed binary code in the PR can make the approval process unreliable. Approvers will check the source code and trust that the ZIP file was properly created. (If the source is tracked elsewhere, then a PR for the ZIP is useless.)\n\n# Leveraging the *archive* Provider to Create the Deployment Package\n\nOne way to not need the deployment package in the repository is with the archive provider. What this provider does is TFC will create the deployment package during the apply. This simplifies the PRs because it contains source code that approvers can check. In the main.tf, you must add the archive_file data source and modify the aws_lambda_function resource to reference the relevant archive_file attributes. The configuration file should look like this:\n\n``` default\nresource \"aws_lambda_function\" \"this\" { \n    ...\n    filename         = data.archive_file.demo_lambda_zip.output_path \n    source_code_hash = data.archive_file.this.output_base64sha256 \n    ...\n}\n```\n\nand\n\n``` default\ndata \"archive_file\" \"this\" { \n    ...\n    type        = \"zip\" \n    source_file = \"${path.module}/src/application.py\"  \n    output_path = \"${path.module}/zip/deployment_package.zip\" \n    ...\n}\n```\n\nUsing the archive provider addressed your previous issue of not needing to include the deployment package in the repository. Unfortunately, does not offer a good advantage if the lambda requires dependencies, and the most common dependencies I have seen for lambda deployments are *requests* and *boto3*. If you decide to add the dependencies with the source code, the pull request will contain too many files, making the approvers' job more difficult and running limitations of the version control GUI to list all the changes.\n\n# Storing the Deployment Package in a S3 Bucket\n\nThe other option for uploading the deployment package into the AWS Lambda service is a S3 bucket. The S3 bucket stores the deployment package, and you configure the corresponding arguments in the aws_s3_bucket resource to use the bucket.\n\nBe sure to create the bucket in a separate terraform deployment. If you include the aws_s3_bucket resource together with the rest of the resources in the main.tf, you will run into drifting again.\n\n``` default\nresource \"aws_lambda_function\" \"lambda\" { \n    ...\n    s3_bucket        = \"bucket-deployment-package-lambda\" \n    s3_key           = \"deployment_package.zip\" \n    source_code_hash = base64sha256(file(\"deployment_package.zip\")) \n    ... \n}\n```\n\nI prefer using the *aws_s3_object* data source instead to take advantage of its arguments; thus, the code looks more like this:\n\n``` default\nresource \"aws_lambda_function\" \"lambda\" { \n    ...\n    s3_bucket        = data.aws_s3_object.lambda.bucket \n    s3_key           = data.aws_s3_object.lambda.key \n    source_code_hash = data.aws_s3_object.lambda.etag\n    ... \n}\n```\n\n``` default\ndata \"aws_s3_object\" \"lambda\" { \n    ...\n    bucket = \"bucket-deployment-package-lambda\" \n    key    = \"deployment_package.zip\"\n    ...\n}\n```\n\nThis approach separates the lambda code from the terraform configuration files. Now, it is easier to develop automation for the lambda code that does not conflict with Terraform Cloud workflow.\n\n# Working with S3 Buckets Encryption\n\nMy success was short-lived for the reason that I overlooked encrypting the bucket. The side effect of an encrypted bucket is the etag attribute fails because the MD5 function can no longer access the file's content. Luckily, the change is trivial and only requires replacing this argument with *s3_object_version*; versioning must also be enabled.\n\n``` default\nresource \"aws_lambda_function\" \"lambda\" { \n    ...\n    s3_bucket         = data.aws_s3_object.lambda.bucket \n    s3_key            = data.aws_s3_object.lambda.key \n    s3_object_version = data.aws_s3_object.lambda.version_id \n    ... \n}\n```\n\n# Using Lambda Layers to the Deployment\n\nYou probably already saw the following message when you tried to view the Lambda console code editor: *The deployment package of your Lambda function REDACTED is too large to enable inline code editing...* The most probable culprits are the dependencies making the deployment package larger than 3MB. It is acceptable to leave the lambda as it is; however, having access to the source code is convenient for quick troubleshooting.\n\nUsing lambda layers can provide help. Instead of bundling the lambda code with dependencies, you can create two deployment packages: one for the lambda code and the other one for your lambda layer. Both can be stored in the same S3 bucket if preferred.\n\nFirst, I add the *aws_lambda_layer_version resource* to the *main.tf* to create the layer:\n\n``` default\nresource \"aws_lambda_layer_version\" \"layer\" {\n    ...\n    layer_name          = \"lambda_layer_name\"\n    s3_bucket           = data.aws_s3_object.layer.bucket\n    s3_key              = data.aws_s3_object.layer.key\n    s3_object_version   = data.aws_s3_object.layer.version_id\n    compatible_runtimes = [\"python3.11\"]\n    ...\n}\n```\n\n``` default\ndata \"aws_s3_object\" \"layer\" {\n    ...\n    bucket = \"bucket-deployment-package-lambda\"\n    key    = \"layer_deployment_package.zip\"\n    ...\n}\n```\n\nThen, I changed the *aws_lambda_function* resource to include a reference to the layer.\n\n``` default\nresource \"aws_lambda_function\" \"lambda\" {\n    ...\n    layers = [aws_lambda_layer_version.layer.arn]\n    ...\n}\n```\n\nAside from the extra complexity and developing time, lambda layers bring benefits. It reduces the size of your deployment packages, separates core function logic from dependencies, shares dependencies, and allows using the Lambda console code editor.\n\nI think it is important to note that you can manage independent pipelines for each main component: terraform configuration files, lambda code, and dependencies. There is no need to create a one-size-fits-all-convoluted pipeline. Test scripts can be simpler and testing should be faster since you only need to check the item changed as opposed to the entire deployment.\n\n# In the end...\n\nIn the end, you and your team need to evaluate which strategy works the best in your environment based on workflow, toolings, knowledge, etc. My idea here is to provide a comparison of the different approaches I have done so far.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}